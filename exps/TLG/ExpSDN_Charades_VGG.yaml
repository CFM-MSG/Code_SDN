##config resume switcher
# please use command parameter to resume config, do not insert in yaml file
# yaml > resume > command parameters

## dataset block:dataset\tranforms\dataloader

loader_config:
  train:
    dataset: "charades"
    split: "train"
    data_dir: /mnt/data3/jiangxun/Charades/
    features_path: /mnt/data3/jiangxun/Charades/vgg_feature/vgg_rgb_features.hdf5
    ann_file_path: /mnt/data3/jiangxun/Charades/vgg_feature/train.json
    embeddings_path: /mnt/data3/jiangxun/Charades/word_embeddings/glove.840B.300d.txt
    feature_sample_num: -1
    min_count: 1
    train_max_length: 30
    test_max_length: 30
    batch_size: 8
    num_workers: 4

  eval: 
    dataset: "charades"
    split: "val"
    data_dir: /mnt/data3/jiangxun/Charades/
    features_path: /mnt/data3/jiangxun/Charades/vgg_feature/vgg_rgb_features.hdf5
    ann_file_path: /mnt/data3/jiangxun/Charades/vgg_feature/test.json
    embeddings_path: /mnt/data3/jiangxun/Charades/word_embeddings/glove.840B.300d.txt
    feature_sample_num: -1
    min_count: 1
    train_max_length: 30
    test_max_length: 30
    batch_size: 32
    num_workers: 4

  test: 
    dataset: "charades"
    split: "test"
    data_dir: /mnt/data3/jiangxun/Charades/
    features_path: /mnt/data3/jiangxun/Charades/vgg_feature/vgg_rgb_features.hdf5
    ann_file_path: /mnt/data3/jiangxun/Charades/vgg_feature/test.json
    embeddings_path: /mnt/data3/jiangxun/Charades/word_embeddings/glove.840B.300d.txt
    feature_sample_num: -1
    min_count: 1
    train_max_length: 30
    test_max_length: 30
    batch_size: 32
    num_workers: 4

# root: ../data/seq2seq/
# dict_loadpath: ./corpus_dicts.pkl


## model config block: backbone\modeltype\using load?
## backbone: in framework, support vgg only temporarily
model:
  arch: ExpSDN
  arch_params: 
    feature_sample_num: -1
    feature_input_dim: 4096
    #SDM
    decoupling_scene_width: 1
    decoupling_action_width: 2
    decoupling_event_width: 4
    decoupling_scene_step: 1
    decoupling_action_step: 2
    decoupling_event_step: 3
    #SMB
    modeling_scene_width: 11
    modeling_scene_depth: 5
    modeling_action_width: 7
    modeling_action_depth: 3
    modeling_event_width: 3
    modeling_event_depth: 1
    #feature presentation
    no_attw_regression: False # disable the attentive regression may increase the accuracy sometimes

  loss_type: sdn_loss
  loss_params: 
    USE_ATTENTION_LOSS: True
    USE_MID_LOSS: True


## some model parameters is wider than current model.use load_no_strict to igore extra parameters.
load_no_strict: false

## process switchers block: the task output\epoch
epochs: 30
eval_epoch: 1
train_log: ./log/train_SDN_charades_vgg.txt
test_log: ./log/test_SDN_charades_vgg.txt
output: ./results
checkpoint_save: charades_vgg.ckpt
best_model_save: charades_vgg.best

## optimizer block
optimizer: adam
weight_decay: 1.0e-5
grad_clip: 5
lr: 4.0e-5

no_wd_bias: True
bias_lr_factor: 2 # used for bias param in scheduler when no_wd_bias is enabled (get_lr()*bias_lr_factor)

# optimizer_cfg:  #dict, extra config for optimizer, eg. Adam: betas([float, float]), eps(float)
lr_methods: ['fix'] 
lr_starts: [1] 
lr_ends:   [1, 1] 
lr_steps:  [30] 

## observation block
#logger
print_frequency: 400
#visalization
vis: False
vis_method: tensorboard
vis_port: 6006


## test
seed: 0