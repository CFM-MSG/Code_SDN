##config resume switcher
# please use command parameter to resume config, do not insert in yaml file
# yaml > resume > command parameters

## dataset block:dataset\tranforms\dataloader

loader_config:
  train:
    dataset: "charades"
    split: "train"
    data_dir: $path to save *.pkl$
    features_path: $dir to *.npy$
    ann_file_path: $path to *.json$
    embeddings_path: $path to glove embedding files$
    feature_sample_num: 1000
    min_count: 1
    train_max_length: 30
    test_max_length: 30
    batch_size: 16
    num_workers: 4

  eval: 
    dataset: "charades"
    split: "val"
    data_dir: $path to save *.pkl$
    features_path: $dir to *.npy$
    ann_file_path: $path to *.json$
    embeddings_path: $path to glove embedding files$
    feature_sample_num: 1000
    min_count: 1
    train_max_length: 30
    test_max_length: 30
    batch_size: 32
    num_workers: 4

  test: 
    dataset: "charades"
    split: "test"
    data_dir: $path to save *.pkl$
    features_path: $dir to *.npy$
    ann_file_path: $path to *.json$
    embeddings_path: $path to glove embedding files$
    feature_sample_num: 1000
    min_count: 1
    train_max_length: 30
    test_max_length: 30
    batch_size: 32
    num_workers: 4

## model config block: backbone\modeltype\using load?
## backbone: in framework, support vgg only temporarily
model:
  arch: ExpSDN
  arch_params: 
    feature_sample_num: 1000
    feature_input_dim: 1024
    #SDM
    decoupling_scene_width: 1
    decoupling_action_width: 2
    decoupling_event_width: 4
    decoupling_scene_step: 1
    decoupling_action_step: 2
    decoupling_event_step: 3
    #SMB
    modeling_scene_width: 11
    modeling_action_width: 7
    modeling_event_width: 3
    modeling_scene_depth: 5
    modeling_action_depth: 3
    modeling_event_depth: 1
    #feature presentation
    no_attw_regression: False # disable the attentive regression may increase the accuracy sometimes

  loss_type: sdn_loss
  loss_params: 
    USE_ATTENTION_LOSS: True
    USE_MID_LOSS: True


## some model parameters is wider than current model.use load_no_strict to igore extra parameters.
load_no_strict: true

## process switchers block: the task output\epoch
epochs: 30
eval_epoch: 1
train_log: ./log/train_SDN_charades_i3d.txt
test_log: ./log/test_SDN_charades_i3d.txt
output: ./results
checkpoint_save: charades_i3d.ckpt
best_model_save: charades_i3d.best

## optimizer block
optimizer: adam
weight_decay: 1.0e-5
grad_clip: 5
lr: 4.0e-5

no_wd_bias: True
bias_lr_factor: 2 # used for bias param in scheduler when no_wd_bias is enabled (get_lr()*bias_lr_factor)

# optimizer_cfg:  #dict, extra config for optimizer, eg. Adam: betas([float, float]), eps(float)
lr_methods: ['fix']
lr_starts: [1]
lr_ends:   [1]
lr_steps:  [30]

## observation block
#logger
print_frequency: 400
#visalization
vis: False
vis_method: tensorboard
vis_port: 6006


## test
seed: 0